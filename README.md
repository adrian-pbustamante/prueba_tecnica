## ðŸ“Œ Project Overview

The goal is twofold:

1. Build **classification models** to predict whether a client will default on their next payment.
2. Apply a **hurdle modeling approach** to estimate the continuous variable 'PAY_AMT4'.

The dataset comes from the **Default of Credit Card Clients in Taiwan (UCI Repository)**.
https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients

---

## ðŸ—‚ï¸ Repository Structure

```
credit-default-hurdle-model/
â”‚
â”œâ”€â”€ notebooks/                # Exploratory & modeling notebooks
â”‚   â”œâ”€â”€ eda_processing.ipynb
â”‚   â”œâ”€â”€ classification.ipynb
â”‚   â””â”€â”€ Regression.ipynb
â”‚
â”œâ”€â”€ src/                      
â”‚   â”œâ”€â”€ data/preprocessing.py
â”‚   â”œâ”€â”€ models/train_classification.py
â”‚   â”œâ”€â”€ models/train_hurdle.py
â”‚   â”œâ”€â”€ models/evaluate.py
â”‚   â””â”€â”€ utils/save_load.py
â”‚
â”œâ”€â”€ data/                     # Dataset (raw & processed)
â”‚   â”œâ”€â”€ default_of_credit_card_clients.csv
â”‚   â””â”€â”€ df_encoded.csv
â”‚
â”œâ”€â”€ models/                   # Trained models
â”‚   â”œâ”€â”€ logreg.pkl
â”‚   â”œâ”€â”€ xgb_smotetomek.pkl
â”‚   â”œâ”€â”€ xgb_grid.pkl
â”‚   â”œâ”€â”€ hurdle_classifier.pkl
â”‚   â”œâ”€â”€ hurdle_regressor_ridge.pkl
â”‚   â””â”€â”€ hurdle_regressor_catboost.pkl
â”‚
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ README.md                 # Project overview
â””â”€â”€ .gitignore
```

---

## Methodology

### 1. Data Exploration & Preprocessing

* Analyzed dataset distributions and correlations.
* Handled missing values and categorical encoding.

### 2. Classification Models

Trained multiple classifiers to predict `default.payment.next.month`:

* **Logistic Regression** (with SMOTETomek resampling)
* **XGBoost** (baseline and with SMOTETomek resampling)

**Evaluation Metrics**:

* F1-score
* Precision & Recall
* ROC-AUC

**Key Finding**:

* XGBoost + SMOTETomek achieved the highest recall, making it more suitable since minimizing false negatives is crucial in credit risk.
* Logistic Regression remains competitive if interpretability is prioritized.

### 3. Hurdle Regression

Implemented a two-part model to predict 'PAY_AMT4'

1. **Classification step** â†’ predicts whether 'PAY_AMT4' is zero or not.
2. **Regression step** â†’ estimates 'PAY_AMT4', conditional to 'PAY_AMT4'>0.

Models:

* **Ridge Regression**
* **CatBoost Regressor**

---

## ðŸ“Š Results Summary

### Classification Results 

| Model                | ROC-AUC    | F1         | Precision | Recall     |
| -------------------- | ---------- | ---------- | --------- | ---------- |
| Logistic Regression  | 0.745      | 0.519      | 0.50      | 0.54       |
| XGBoost              | 0.766      | 0.526      | 0.50      | 0.55       |
| XGBoost + SMOTETomek | 0.764      | 0.527      | 0.49      | 0.57       |

The **XGBoost + SMOTETomek** model provides the best recall, crucial for catching risky clients.
Logistic Regression  + SMOTETomak is solid

### Rgression results

 Clasificador (Hurdle):
 
| Accuracy: 0.914 | F1: 0.750 | ROC-AUC: 0.858 |

Regression:                                                                  

|   Modelo        | MAE     | RMSE     | R2    |
|-----------------|---------|----------|-------|        
| Ridge Baseline  | 4568.59 | 11873.91 | 0.139 |
| Ridge Hurdle    | 4389.10 | 11757.94 | 0.156 |
| CatBoost Hurdle | 4461.09 | 11597.75 | 0.178 |


* The **classification step** improves compared baseline models.
* In the **regression step**, CatBoost showed stronger predictive power than Ridge.
* The hurdle framework is promising, but the regression step needs to be imporved.

---
